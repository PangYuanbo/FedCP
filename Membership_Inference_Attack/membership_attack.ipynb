{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "bc7b3873ced32b9f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-21T18:23:04.317253Z",
     "start_time": "2025-02-21T18:23:01.545717Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import copy\n",
    "import torch.optim as optim\n",
    "import os\n",
    "\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset, Subset\n",
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from model import *\n",
    "\n",
    "# ==============================\n",
    "# 一些超参数 & 配置\n",
    "# ==============================\n",
    "BATCH_SIZE = 10\n",
    "EPOCHS = 800   # 为了演示，训练回合数较小，实际可调大一些\n",
    "LR = 1e-3\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# ===================================================\n",
    "# 1. 定义一个简单的 MLP，用于多分类 (target/shadow)\n",
    "# ===================================================\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = FedAvgCNN(in_features=1, num_classes=10, dim=1024).to(DEVICE)\n",
    "model_head = copy.deepcopy(model.fc)\n",
    "model.fc = nn.Identity()\n",
    "model = LocalModel(model, model_head)\n",
    "in_dim = list(model.head.parameters())[0].shape[1]\n",
    "cs = ConditionalSelection(in_dim, in_dim).to(DEVICE)\n",
    "\n",
    "model = Ensemble(\n",
    "        model=copy.deepcopy(model),\n",
    "        cs=copy.deepcopy(cs),\n",
    "        head_g=copy.deepcopy(model.head),  # head is the global head\n",
    "        feature_extractor=copy.deepcopy(model.feature_extractor)\n",
    "        # feature_extractor is the global feature_extractor\n",
    ")\n",
    "target_model=  copy.deepcopy(model)\n",
    "shadow_model = copy.deepcopy(model)"
   ],
   "id": "cf5ae39d7b6bb2a",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-21T18:23:23.482260Z",
     "start_time": "2025-02-21T18:23:23.475256Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "def read_data(is_train=True,is_shadow=True):\n",
    "    \"\"\"\n",
    "    读取 train0_.npz ~ train19_.npz 或 test0_.npz ~ test19_.npz，\n",
    "    并将它们的 'data'（字典）按键进行拼接，最终返回一个“字典”结构。\n",
    "    可以像原来那样使用 train_data['x'] 获取合并后的数据。\n",
    "    \"\"\"\n",
    "    # 用于存放合并后数据的容器，格式：{key1: [array1, array2, ...], key2: [...], ...}\n",
    "    merged_dict = {}\n",
    "\n",
    "    for i in range(20):\n",
    "        if is_shadow:\n",
    "            file_name = f\"{'train_shadow' if is_train else 'test_shadow'}{i}_.npz\"\n",
    "        else:\n",
    "            file_name = f\"{'train' if is_train else 'test'}{i}_.npz\"\n",
    "        if not os.path.exists(file_name):\n",
    "            raise FileNotFoundError(f\"File {file_name} not found.\")\n",
    "\n",
    "        # 1. 读取单个文件中的字典\n",
    "        with open(file_name, 'rb') as f:\n",
    "            single_data = np.load(f, allow_pickle=True)['data'].tolist()\n",
    "            # single_data 应该是一个字典，如 {'x': np.array(...), 'y': np.array(...), ...}\n",
    "\n",
    "        # 2. 将 single_data 的键值，合并到 merged_dict 中\n",
    "        for key, value in single_data.items():\n",
    "            # 若在 merged_dict 中还没有这个 key，就初始化为一个空列表\n",
    "            if key not in merged_dict:\n",
    "                merged_dict[key] = []\n",
    "            # 把当前文件的 value 追加进列表\n",
    "            merged_dict[key].append(value)\n",
    "\n",
    "    # 3. 把每个 key 对应的列表都做一次拼接 (np.concatenate)，得到单个数组\n",
    "    final_dict = {}\n",
    "    for key, list_of_arrays in merged_dict.items():\n",
    "        # 假设这些数组的形状在第 0 维可以拼接\n",
    "        # 如果有的键是标量或不需拼接，需自己定制逻辑\n",
    "        final_dict[key] = np.concatenate(list_of_arrays, axis=0)\n",
    "\n",
    "    return final_dict\n",
    "\n",
    "\n",
    "def read_client_data(is_train=True,is_shadow=True):\n",
    "    # 如果 dataset 中包含其他情况，比如 News / Shakespeare，需要你自己实现\n",
    "    # 这里只演示默认读取\n",
    "    if is_train:\n",
    "        train_data = read_data( is_train=True,is_shadow=is_shadow)\n",
    "        X_train = torch.Tensor(train_data['x']).type(torch.float32)\n",
    "        y_train = torch.Tensor(train_data['y']).type(torch.int64)\n",
    "        train_data = [(x, y) for x, y in zip(X_train, y_train)]\n",
    "        return TensorDataset(X_train, y_train)\n",
    "    else:\n",
    "        test_data = read_data(is_train=False,is_shadow=is_shadow)\n",
    "        X_test = torch.Tensor(test_data['x']).type(torch.float32)\n",
    "        y_test = torch.Tensor(test_data['y']).type(torch.int64)\n",
    "        test_data = [(x, y) for x, y in zip(X_test, y_test)]\n",
    "        return TensorDataset(X_test, y_test)"
   ],
   "id": "2e8edcc7656494a2",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ===================================================\n",
    "# # 2. 定义一个简单的 MLP，用于二分类 (attack model)\n",
    "# # ===================================================\n",
    "class AttackMLP(nn.Module):\n",
    "    def __init__(self, input_dim=10, hidden_dim=32, output_dim=2):\n",
    "        super(AttackMLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x  # logits\n"
   ],
   "id": "29503d8b4fe9364"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-21T18:23:04.377253Z",
     "start_time": "2025-02-21T18:23:04.370253Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# ===================================================\n",
    "# 3. 训练/评估函数\n",
    "# ===================================================\n",
    "\n",
    "def build_attack_loader(data, batch_size):\n",
    "    \"\"\"\n",
    "    构造 PyTorch DataLoader\n",
    "\n",
    "    每条数据格式为：[pred_vector, true_label, membership_label]\n",
    "    其中 pred_vector 是预测向量，true_label 是真实标签，\n",
    "    membership_label 表示成员/非成员标签。\n",
    "\n",
    "    将 pred_vector 与 true_label 拼接成一个特征向量。\n",
    "\n",
    "    参数:\n",
    "        data: list，每个元素为一条记录\n",
    "        batch_size: 批次大小\n",
    "\n",
    "    返回:\n",
    "        DataLoader 对象\n",
    "    \"\"\"\n",
    "    # 提取特征向量：拼接预测向量和真实标签\n",
    "    X = np.array([np.concatenate([record[0], [record[1]]]) for record in data])\n",
    "    # 提取成员/非成员标签\n",
    "    y = np.array([record[2] for record in data])\n",
    "    # 构造 TensorDataset 和 DataLoader\n",
    "    dataset = TensorDataset(torch.from_numpy(X).float(), torch.from_numpy(y).long())\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    return loader\n",
    "def train_one_epoch(model, dataloader, optimizer, device=DEVICE):\n",
    "    model.train()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    total_loss = 0.0\n",
    "    for batch_x, batch_y in dataloader:\n",
    "        batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "        # 前向\n",
    "        logits = model(batch_x)\n",
    "        loss = criterion(logits, batch_y)\n",
    "        # 反向传播\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def evaluate(model, dataloader, device=DEVICE):\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y in dataloader:\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "            logits = model(batch_x)\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            correct += (preds == batch_y).sum().item()\n",
    "            total += len(batch_y)\n",
    "    return correct / total\n",
    "\n",
    "def evaluate_attack(loader, model, device):\n",
    "    \"\"\"\n",
    "    对给定的 DataLoader 中数据利用攻击模型进行评估，返回准确率。\n",
    "\n",
    "    参数:\n",
    "        loader: DataLoader，包含了待评估的数据（输入及对应标签）\n",
    "        model: 攻击模型\n",
    "        device: 设备（例如 torch.device(\"cuda\") 或 torch.device(\"cpu\")）\n",
    "\n",
    "    返回:\n",
    "        accuracy: 预测的准确率\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    preds_list = []\n",
    "    labels_list = []\n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y in loader:\n",
    "            batch_x = batch_x.to(device)\n",
    "            logits = model(batch_x)  # 输出 shape 为 (batch_size, 2)\n",
    "            preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "            preds_list.append(preds)\n",
    "            labels_list.append(batch_y.numpy())\n",
    "    # 合并所有批次的预测结果和真实标签\n",
    "    y_pred = np.concatenate(preds_list, axis=0)\n",
    "    y_true = np.concatenate(labels_list, axis=0)\n",
    "    # 计算准确率\n",
    "    return accuracy_score(y_true, y_pred)"
   ],
   "id": "1e35717fa830636b",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def membership_inference_attack_pipeline(name, target_model, target_label, BATCH_SIZE, DEVICE, attack_model,\n",
    "                                         num_clients=5):\n",
    "    \"\"\"\n",
    "    Performs the complete membership inference attack pipeline:\n",
    "    1. Aggregates client models and loads them into the target model.\n",
    "    2. Loads the shadow model parameters.\n",
    "    3. Prepares training and holdout datasets (filtered by target_label).\n",
    "    4. Constructs attack data and builds corresponding DataLoaders.\n",
    "    5. Evaluates the attack model on the overall dataset (members + non-members),\n",
    "       on member-only data (TPS), and non-member-only data (FPS).\n",
    "    6. Computes and prints evaluation metrics including the combined metric.\n",
    "\n",
    "    Parameters:\n",
    "        name (str): Used in the client file naming.\n",
    "        target_model (torch.nn.Module): Model instance to load parameters into.\n",
    "        target_label (int): Label for filtering the datasets.\n",
    "        BATCH_SIZE (int): Batch size for DataLoaders.\n",
    "        DEVICE (torch.device or str): Device for torch operations.\n",
    "        attack_model: Attack model used for evaluation.\n",
    "        num_clients (int): Number of client files to average (default is 5).\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing evaluation metrics and attack DataLoaders.\n",
    "    \"\"\"\n",
    "\n",
    "    # ===== Step 1: Aggregate client models =====\n",
    "    client_files = [\n",
    "        f'results_mnist-0.1-normal_client{i}_1000_0.0050{name}.pt'\n",
    "        for i in range(num_clients)\n",
    "    ]\n",
    "\n",
    "    avg_state_dict = None\n",
    "    for client_file in client_files:\n",
    "        client_state_dict = torch.load(client_file, map_location=DEVICE,weights_only=True)\n",
    "        if avg_state_dict is None:\n",
    "            avg_state_dict = OrderedDict()\n",
    "            for key, param in client_state_dict.items():\n",
    "                avg_state_dict[key] = param.clone()  # Initialize with the first client's parameters\n",
    "        else:\n",
    "            for key, param in client_state_dict.items():\n",
    "                avg_state_dict[key] += param\n",
    "\n",
    "    # Take average of the parameters.\n",
    "    for key in avg_state_dict.keys():\n",
    "        avg_state_dict[key] /= num_clients\n",
    "\n",
    "    # Load the averaged parameters into target_model.\n",
    "    target_model.load_state_dict(avg_state_dict)\n",
    "    # If you prefer to load shadow model parameters directly, you could do:\n",
    "    # target_model.load_state_dict(torch.load('shadow_model.pth'))\n",
    "\n",
    "    # ===== Step 2: Prepare datasets and DataLoaders =====\n",
    "    train_dataset = read_client_data(is_train=True, is_shadow=False)\n",
    "    holdout_dataset = read_client_data(is_train=False, is_shadow=False)\n",
    "\n",
    "    # Define indices based on the holdout dataset length.\n",
    "    train_indices = list(range(len(holdout_dataset)))\n",
    "    train_dataset = Subset(train_dataset, train_indices)\n",
    "\n",
    "    # Filter datasets: keep only samples with target_label.\n",
    "    train_dataset_filtered = filter_by_label(train_dataset, target_label)\n",
    "    holdout_dataset_filtered = filter_by_label(holdout_dataset, target_label)\n",
    "\n",
    "    # Create DataLoaders.\n",
    "    train_loader = DataLoader(train_dataset_filtered, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    holdout_loader = DataLoader(holdout_dataset_filtered, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    # Load shadow model parameters (overwriting the averaged ones, if needed).\n",
    "    # target_model.load_state_dict(torch.load('shadow_model.pth'))\n",
    "\n",
    "    # ===== Step 3: Obtain model outputs and construct attack data =====\n",
    "    train_probs, train_true_labels = get_model_outputs_with_labels(target_model, train_loader)\n",
    "    holdout_probs, holdout_true_labels = get_model_outputs_with_labels(target_model, holdout_loader)\n",
    "\n",
    "    # Construct attack data as tuples: (predicted probability, true label, membership_flag)\n",
    "    train_data = [(y_pred, y_true, 1) for y_pred, y_true in zip(train_probs, train_true_labels)]\n",
    "    holdout_data = [(y_pred, y_true, 0) for y_pred, y_true in zip(holdout_probs, holdout_true_labels)]\n",
    "\n",
    "    # Merge data: test_data contains both members and non-members.\n",
    "    test_data = train_data + holdout_data\n",
    "    test_TPS = train_data  # TPS: member data\n",
    "    test_FPS = holdout_data  # FPS: non-member data\n",
    "\n",
    "    # Build DataLoaders for the attack data.\n",
    "    attack_test_loader = build_attack_loader(test_data, BATCH_SIZE)\n",
    "    attack_TPS_loader = build_attack_loader(test_TPS, BATCH_SIZE)\n",
    "    attack_FPS_loader = build_attack_loader(test_FPS, BATCH_SIZE)\n",
    "\n",
    "    # ===== Step 4: Evaluate the attack model =====\n",
    "    # 1. Evaluate on the merged dataset (members + non-members)\n",
    "    attack_acc = evaluate_attack(attack_test_loader, attack_model, DEVICE)\n",
    "    print(f\"[Membership Inference Attack] Accuracy: {attack_acc:.4f}\")\n",
    "\n",
    "    # 2. Evaluate on TPS (member data)\n",
    "    TPS_acc = evaluate_attack(attack_TPS_loader, attack_model, DEVICE)\n",
    "    print(f\"[Membership Inference Attack(TPS)] Accuracy: {TPS_acc:.4f}\")\n",
    "\n",
    "    # 3. Evaluate on FPS (non-member data)\n",
    "    FPS_acc = evaluate_attack(attack_FPS_loader, attack_model, DEVICE)\n",
    "    FPS_error = 1.0 - FPS_acc\n",
    "    print(f\"[Membership Inference Attack(FPS)] Error: {FPS_error:.4f}\")\n",
    "\n",
    "    # Compute the combined metric:\n",
    "    # Note: Since FPS_acc = 1 - FPS_error, the combined metric is equivalent to:\n",
    "    # (len(test_TPS)*TPS_acc + len(test_FPS)*FPS_acc) / len(test_data)\n",
    "    combined_metric = (len(test_TPS) * TPS_acc + len(test_FPS) * FPS_acc) / len(test_data)\n",
    "    print(f\"Combined Metric: {combined_metric:.4f}\")\n",
    "\n",
    "    # Return a dictionary of evaluation metrics and loaders.\n",
    "    return {\n",
    "        'attack_acc': attack_acc,\n",
    "        'TPS_acc': TPS_acc,\n",
    "        'FPS_acc': FPS_acc,\n",
    "        'FPS_error': FPS_error,\n",
    "        'combined_metric': combined_metric,\n",
    "        'attack_test_loader': attack_test_loader,\n",
    "        'attack_TPS_loader': attack_TPS_loader,\n",
    "        'attack_FPS_loader': attack_FPS_loader\n",
    "    }"
   ],
   "id": "40ffff7226df69"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-21T18:23:49.794705Z",
     "start_time": "2025-02-21T18:23:49.371706Z"
    }
   },
   "cell_type": "code",
   "source": [
    "shadow_train_dataset = read_client_data(is_train=True,is_shadow=True)\n",
    "shadow_holdout_dataset = read_client_data(is_train=False,is_shadow=True)\n",
    "shadow_train_loader = DataLoader(shadow_train_dataset, drop_last=True,batch_size=BATCH_SIZE, shuffle=False)\n",
    "shadow_holdout_loader = DataLoader(shadow_holdout_dataset,drop_last=True, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# 影子模型 (与目标模型结构相同)\n",
    "#\n",
    "checkpoint = torch.load('results_mnist-0.1-normal_client0_1000_0.0050.pt', map_location=DEVICE)\n",
    "partial_state_dict = {}\n",
    "for k, v in checkpoint.items():\n",
    "    if \"gate\" in k:\n",
    "        partial_state_dict[k] = v\n",
    "shadow_model.load_state_dict(partial_state_dict, strict=False)\n",
    "\n",
    "\n",
    "for param in shadow_model.gate.parameters():\n",
    "    param.requires_grad = False\n",
    "optimizer_s = optim.SGD(shadow_model.parameters(), lr=LR)\n",
    "\n",
    "# 训练影子模型\n",
    "# for epoch in range(EPOCHS):\n",
    "#     loss_s = train_one_epoch(shadow_model, shadow_train_loader, optimizer_s, device=DEVICE)\n",
    "#     acc_s_train = evaluate(shadow_model, shadow_train_loader, device=DEVICE)\n",
    "#     acc_s_holdout = evaluate(shadow_model, shadow_holdout_loader, device=DEVICE)\n",
    "#     print(f\"[Shadow Model] Epoch {epoch+1}/{EPOCHS}, Loss: {loss_s:.4f}, \"\n",
    "#           f\"Train Acc: {acc_s_train:.4f}, Shadow-Holdout Acc: {acc_s_holdout:.4f}\")\n",
    "# torch.save(shadow_model.state_dict(), 'shadow_model.pth')\n",
    "\n",
    "\n",
    "\n",
    "checkpoint2= torch.load('shadow_model.pth', map_location=DEVICE)\n",
    "shadow_model.load_state_dict(checkpoint2)"
   ],
   "id": "8865ac2d9a2a9913",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aaron\\AppData\\Local\\Temp\\ipykernel_47236\\2510091026.py:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load('results_mnist-0.1-normal_client0_1000_0.0050.pt', map_location=DEVICE)\n",
      "C:\\Users\\aaron\\AppData\\Local\\Temp\\ipykernel_47236\\2510091026.py:31: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint2= torch.load('shadow_model.pth', map_location=DEVICE)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-21T18:24:23.920833Z",
     "start_time": "2025-02-21T18:24:23.903314Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def filter_by_label(dataset, target_label):\n",
    "    \"\"\"\n",
    "    过滤数据集中的样本，仅保留标签为 target_label 的数据。\n",
    "    \"\"\"\n",
    "    filtered_indices = [i for i, (_, label) in enumerate(dataset) if label == target_label]\n",
    "    return Subset(dataset, filtered_indices)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_model_outputs_with_labels(model, dataloader):\n",
    "    \"\"\"\n",
    "    获取模型对数据的预测向量和对应的真实标签。\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    outputs_list = []\n",
    "    labels_list = []\n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y in dataloader:\n",
    "            batch_x = batch_x.to(DEVICE)\n",
    "            logits = model(batch_x)  # (batch_size, 10)\n",
    "            probs = logits  # 未经过 softmax，保留 logits\n",
    "            outputs_list.append(probs.cpu().numpy())\n",
    "            labels_list.append(batch_y.cpu().numpy())  # 获取真实标签\n",
    "    return np.concatenate(outputs_list, axis=0), np.concatenate(labels_list, axis=0)\n",
    "\n",
    "def train_attack_model(shadow_model, shadow_train_dataset,shadow_holdout_dataset,target_label):\n",
    "    # 过滤 shadow_train_dataset 和 shadow_holdout_dataset，仅保留标签为 7 的样本\n",
    "    EPOCHS = 5000\n",
    "\n",
    "    # 攻击模型 (二分类)\n",
    "    attack_model = AttackMLP(input_dim=11, hidden_dim=32, output_dim=2).to(DEVICE)\n",
    "    optimizer_a = optim.Adam(attack_model.parameters(), lr=LR)\n",
    "    shadow_train_dataset_filtered = filter_by_label(shadow_train_dataset, target_label)\n",
    "    shadow_holdout_dataset_filtered = filter_by_label(shadow_holdout_dataset, target_label)\n",
    "\n",
    "    # 创建 DataLoader\n",
    "    # shadow_train_loader = DataLoader(shadow_train_dataset_filtered, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    # shadow_holdout_loader = DataLoader(shadow_holdout_dataset_filtered, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    shadow_train_indices = list(range(len(shadow_holdout_dataset_filtered)))  # 定义需要加载的索引范围\n",
    "    shadow_train_dataset = Subset(shadow_train_dataset_filtered, shadow_train_indices)\n",
    "    shadow_train_loader = DataLoader(shadow_train_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    shadow_holdout_loader = DataLoader(shadow_holdout_dataset_filtered, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    # 获取影子模型的预测概率和真实标签\n",
    "    # 初始化存储列表\n",
    "    all_shadow_train_probs = []\n",
    "    all_shadow_train_labels = []\n",
    "    all_shadow_holdout_probs = []\n",
    "    all_shadow_holdout_labels = []\n",
    "\n",
    "    # 运行多次并拼接数据\n",
    "    num_runs = 20  # 设置运行次数\n",
    "    for _ in range(num_runs):\n",
    "        shadow_train_probs, shadow_train_true_labels = get_model_outputs_with_labels(shadow_model, shadow_train_loader)\n",
    "        shadow_holdout_probs, shadow_holdout_true_labels = get_model_outputs_with_labels(shadow_model,\n",
    "                                                                                         shadow_holdout_loader)\n",
    "\n",
    "        all_shadow_train_probs.append(shadow_train_probs)\n",
    "        all_shadow_train_labels.append(shadow_train_true_labels)\n",
    "        all_shadow_holdout_probs.append(shadow_holdout_probs)\n",
    "        all_shadow_holdout_labels.append(shadow_holdout_true_labels)\n",
    "\n",
    "\n",
    "\n",
    "    # 确保所有数据都是 torch.Tensor\n",
    "    all_shadow_train_probs = [torch.tensor(probs) if isinstance(probs, np.ndarray) else probs for probs in\n",
    "                              all_shadow_train_probs]\n",
    "    all_shadow_train_labels = [torch.tensor(labels) if isinstance(labels, np.ndarray) else labels for labels in\n",
    "                               all_shadow_train_labels]\n",
    "    all_shadow_holdout_probs = [torch.tensor(probs) if isinstance(probs, np.ndarray) else probs for probs in\n",
    "                                all_shadow_holdout_probs]\n",
    "    all_shadow_holdout_labels = [torch.tensor(labels) if isinstance(labels, np.ndarray) else labels for labels in\n",
    "                                 all_shadow_holdout_labels]\n",
    "\n",
    "    # 进行拼接\n",
    "    shadow_train_probs = torch.cat(all_shadow_train_probs, dim=0)\n",
    "    shadow_train_true_labels = torch.cat(all_shadow_train_labels, dim=0)\n",
    "    shadow_holdout_probs = torch.cat(all_shadow_holdout_probs, dim=0)\n",
    "    shadow_holdout_true_labels = torch.cat(all_shadow_holdout_labels, dim=0)\n",
    "\n",
    "    # 构造攻击数据格式 (y_pred, y_true, in/out)\n",
    "    shadow_train_data = [\n",
    "        (y_pred, y_true, 1) for y_pred, y_true in zip(shadow_train_probs, shadow_train_true_labels)\n",
    "    ]\n",
    "    shadow_holdout_data = [\n",
    "        (y_pred, y_true, 0) for y_pred, y_true in zip(shadow_holdout_probs, shadow_holdout_true_labels)\n",
    "    ]\n",
    "\n",
    "    # 合并成员和非成员数据\n",
    "    attack_data = shadow_train_data + shadow_holdout_data\n",
    "\n",
    "    # 转换为 NumPy 格式\n",
    "    X_attack_train = np.array([np.concatenate([record[0], [record[1]]]) for record in attack_data])  # 预测向量 + 真实标签\n",
    "    y_attack_train = np.array([record[2] for record in attack_data])  # 成员/非成员标签\n",
    "\n",
    "    # 构造 PyTorch 数据集\n",
    "    attack_train_dataset = TensorDataset(\n",
    "        torch.from_numpy(X_attack_train).float(),\n",
    "        torch.from_numpy(y_attack_train).long()\n",
    "    )\n",
    "    attack_train_loader = DataLoader(attack_train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "\n",
    "    # 利用攻击模型做预测\n",
    "    attack_model.eval()\n",
    "    preds_list = []\n",
    "    labels_list = []\n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y in attack_train_loader:\n",
    "            batch_x = batch_x.to(DEVICE)\n",
    "            logits = attack_model(batch_x)  # (batch_size, 2)\n",
    "            preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "            preds_list.append(preds)\n",
    "            labels_list.append(batch_y.numpy())\n",
    "\n",
    "    y_pred = np.concatenate(preds_list, axis=0)\n",
    "    y_true = np.concatenate(labels_list, axis=0)\n",
    "\n",
    "    attack_acc = accuracy_score(y_true, y_pred)\n",
    "    print(f\"[Membership Inference Attack] Accuracy: {attack_acc:.4f}\")\n",
    "\n",
    "\n",
    "    attack_model.train()\n",
    "    # 训练攻击模型 (二分类)\n",
    "    for epoch in range(EPOCHS):\n",
    "        loss_a = train_one_epoch(attack_model, attack_train_loader, optimizer_a, device=DEVICE)\n",
    "        # 这里简单地用训练精度衡量\n",
    "        acc_a = evaluate(attack_model, attack_train_loader, device=DEVICE)\n",
    "        print(f\"[Attack Model] Epoch {epoch+1}/{EPOCHS}, Loss: {loss_a:.4f}, Train Acc: {acc_a:.4f}\")\n",
    "\n",
    "    torch.save(attack_model.state_dict(), f'attack_model(single){target_label}.pth')\n",
    "    attack_model.load_state_dict(torch.load(f'attack_model(single){target_label}.pth'))"
   ],
   "id": "e106ed9a76943305",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# for i in range(10):\n",
    "#     train_attack_model(shadow_model, shadow_train_dataset,shadow_holdout_dataset,i)"
   ],
   "id": "94c0c722d8ad59c1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "for target_label in range(10):\n",
    "    print('target_label',target_label)\n",
    "# train_attack_model(shadow_model, shadow_train_dataset,shadow_holdout_dataset,target_label)\n",
    "    attack_model = AttackMLP(input_dim=11, hidden_dim=32, output_dim=2).to(DEVICE)\n",
    "    attack_model.load_state_dict(torch.load(f'attack_model(single)/attack_model(single){target_label}.pth', weights_only=True))\n",
    "    target_model = copy.deepcopy(model)\n",
    "    name0=''\n",
    "    name1='_model.feature_extractor'\n",
    "    name2='_model.head'\n",
    "    membership_inference_attack_pipeline(name0,target_model, target_label, BATCH_SIZE, DEVICE, attack_model)\n",
    "    membership_inference_attack_pipeline(name1,target_model, target_label, BATCH_SIZE, DEVICE, attack_model)\n",
    "    membership_inference_attack_pipeline(name2,target_model, target_label, BATCH_SIZE, DEVICE, attack_model)"
   ],
   "id": "88fcf51dc446e7c5"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
